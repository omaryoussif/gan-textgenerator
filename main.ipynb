{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1805e4d8-c281-448b-8436-e46942ea122f",
   "metadata": {},
   "source": [
    "# Deep Learning Project: Text Generation\n",
    "\n",
    "In this project, I'd like to explore generating realistic-sounding text. After some preliminary research, I'd like to experiemnt doing this through Generative Adversarial Networks (GANs) and Adversarial Autoencoders. The reason I'm doing this is because I'd like to verify how true the hypothesis is that this model is able to do well in text (even though it's primarily used for images and is great at that).\n",
    "\n",
    "For GANs, we will have a competition between the two neural networks: the generator and the discriminator.\n",
    "\n",
    "There has been some research done that explore text generation using GANs (as the normal usage of them revolves around image generation) and it has been found that they're surprisingly good at getting more realistic sounding text compared to other methods. For more realistic human sounding datasets, I chose to use the Amazon Q&A dataset, where it includes all of the questions and answers of products on amazon. This gives me the freedom to also reduce the size to a specific category of products' questions too. I'd like to explore this phenomenon and test it out and see how do results look like and how true that statement is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c48bec-3c20-4b53-ba61-94fa8b98fbda",
   "metadata": {},
   "source": [
    "First, we need to load in the data and do some code to process it into a format that we'll be using later. The original json file is not a valid one, it's a string of Python dictionaries, which makes it need some extra processing. The data came with some clean-up code (create_data.py) that formats it correctly, but unfortauntely only works within the Google Cloud Platform (GCP). Consequrntly, I wrote some Python code that manually converts them into txt files that we can use. The original data is stored in the folder `data` while the cleaned up ones are in `clean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26cdff61-1f15-4fcb-aa79-5e880f91c1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7318255-4194-4833-897f-84a6fe6557c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a text file with a custom number of question/answer pair that samples all text files\n",
    "def generate_random_qa_file(file_path, num_pairs):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = [line.strip() for line in file.readlines()]\n",
    "\n",
    "    qa_pairs = []\n",
    "    current_qa_pair = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line:\n",
    "            current_qa_pair.append(line)\n",
    "        else:\n",
    "            qa_pairs.append(current_qa_pair)\n",
    "            current_qa_pair = []\n",
    "\n",
    "    random.shuffle(qa_pairs)\n",
    "\n",
    "    with open('clean/sample.txt', 'w') as output_file:\n",
    "        for qa_pair in qa_pairs[:num_pairs]:\n",
    "            if len(qa_pair) == 2:\n",
    "                output_file.write(qa_pair[0] + '\\n')\n",
    "                output_file.write(qa_pair[1] + '\\n')\n",
    "                output_file.write('\\n')\n",
    "\n",
    "# This number is the number of pairs you would like to have in the data file named \"sample.txt\" in the folder \"clean\".\n",
    "generate_random_qa_file('clean/full_data.txt', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57016ad2-e592-4ff9-aac2-fa7b28401c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing (to input into training/generation later)\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_path, max_length=100):\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = file.read().lower()\n",
    "        data = re.findall(r'\\b\\w+\\b', data)\n",
    "        self.vocab = sorted(set(data))\n",
    "        if \"<START>\" not in self.vocab:\n",
    "            self.vocab.append(\"<START>\")\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(self.vocab)}       \n",
    "        self.idx_to_word = {idx: word for idx, word in enumerate(self.vocab)}\n",
    "        self.data = [self.word_to_idx[word] for word in data]\n",
    "        print(\"Vocabulary size:\", len(self.vocab))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_input_dim(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a9e45dc-ad0a-435c-9a1c-bc17ea739cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, output_dim, hidden_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(noise_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "     def __init__(self, input_dim, hidden_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "     def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class GAN(nn.Module):\n",
    "    def __init__(self, generator, discriminator):\n",
    "        super(GAN, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.discriminator(self.generator(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e98535a-d738-4a02-b1c9-af9e21a01852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(path = 'clean/sample.txt', model_class=GAN, lr=1e-5, noise_dim=1024, input_dim=5000, hidden_dim=1024, output_dim=5000, max_length=25, embedding_dim=250, epochs=5, batch_size=64):\n",
    "    train_data = TextDataset(path, max_length=max_length)\n",
    "    data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    generator = Generator(noise_dim, train_data.get_input_dim(), hidden_dim)\n",
    "    discriminator = Discriminator(train_data.get_input_dim(), hidden_dim)\n",
    "    loss = nn.BCELoss()\n",
    "\n",
    "    opt_gen = optim.Adam(generator.parameters(), lr=lr)\n",
    "    opt_discrim = optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        for batch in data_loader:\n",
    "            noise = torch.randn(batch_size, noise_dim)\n",
    "            data_real = F.one_hot(batch, num_classes=train_data.get_input_dim()).float()\n",
    "            data_fake = generator(noise)\n",
    "\n",
    "            labels_real = torch.ones((data_real.size(0), 1), device=data_real.device)\n",
    "            labels_fake = torch.zeros((data_fake.size(0), 1), device=data_fake.device)\n",
    "            \n",
    "            output_real = discriminator(data_real)\n",
    "            output_fake = discriminator(data_fake.detach())\n",
    "            \n",
    "            # Training discriminator\n",
    "            opt_discrim.zero_grad()\n",
    "            loss_discrim = loss(output_real, labels_real) + loss(output_fake, labels_fake)\n",
    "            loss_discrim.backward()\n",
    "            opt_discrim.step()\n",
    "    \n",
    "            # Training generator\n",
    "            opt_gen.zero_grad()\n",
    "            labels_real = torch.ones((data_fake.size(0), 1), device=data_fake.device)\n",
    "            loss_gen = loss(discriminator(data_fake), labels_real)\n",
    "            loss_gen.backward()\n",
    "            opt_gen.step()\n",
    "    \n",
    "            # Clear gradients\n",
    "            opt_discrim.zero_grad()\n",
    "            opt_gen.zero_grad()\n",
    "            \n",
    "        if (i+1) % 1 == 0:\n",
    "            print(\"Epoch:\", i, \"Generator Loss:\", loss_gen.item(), \"Discriminator Loss:\", loss_discrim.item())\n",
    "            \n",
    "    return generator\n",
    "\n",
    "# Generate the text\n",
    "def beam_search(generator, noise_dim, train_data, max_length=100, beam_width=20):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        beam = [([], 0.0)]\n",
    "        for _ in range(max_length):\n",
    "            new_beam = []\n",
    "            for seq, log_prob in beam:\n",
    "                output_probs = F.softmax(generator(torch.randn(1, noise_dim)), dim=1)\n",
    "                top_candidates = torch.topk(output_probs, beam_width, dim=1)\n",
    "                for i in range(beam_width):\n",
    "                    token_idx = top_candidates.indices[0][i].item()\n",
    "                    token_prob = top_candidates.values[0][i].item()\n",
    "                    new_seq = seq + [train_data.idx_to_word[token_idx]]\n",
    "                    new_log_prob = np.log(token_prob) + log_prob\n",
    "                    new_beam.append((new_seq, new_log_prob))\n",
    "\n",
    "            new_beam.sort(key=lambda x: -x[1])\n",
    "            beam = new_beam[:beam_width]\n",
    "\n",
    "        best_sequence, _ = beam[0]\n",
    "        generated_text = ' '.join(best_sequence)\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56aebe5e-3a67-463f-9db3-1a3aa5e0aaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5636\n",
      "Epoch: 0 Generator Loss: 1.1007364988327026 Discriminator Loss: 1.1103870868682861\n",
      "Epoch: 1 Generator Loss: 1.043791651725769 Discriminator Loss: 1.1330885887145996\n",
      "Epoch: 2 Generator Loss: 1.1323392391204834 Discriminator Loss: 1.084793210029602\n",
      "Epoch: 3 Generator Loss: 1.187387466430664 Discriminator Loss: 1.0572293996810913\n",
      "Epoch: 4 Generator Loss: 1.3126481771469116 Discriminator Loss: 1.0036396980285645\n"
     ]
    }
   ],
   "source": [
    "path = 'clean/sample.txt'\n",
    "generator = train(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9461c60-9829-4c84-90ba-9be2a07bef27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5636\n",
      "Generated Text: 1996 corners extra expert spite corners golf dedicated include inquiring corners squeeze catch wave hook had unlocking had video had had real double had wave group mx20 matters process cleanable incoming corners roofing had had varies anymore few dish had xl34 speakers had wheelchairs diffuser anymore incoming had ties fibrous had s5s incoming i435 corners allows suddenly ideal scented weekly developing channel fibrous suddenly improved had 5lb catch monitor had i435 golf wave wrapper suddenly hawaiian ringger golf lleg had concerning othe wheelchairs porch squeeze columbia draw suddenly equipment ohm andrew had cleanable cropped incoming consoles diffuser roofing louder dish\n"
     ]
    }
   ],
   "source": [
    "generated_text = beam_search(generator, noise_dim=1024, train_data=TextDataset(path), max_length = 100, beam_width = 20)\n",
    "print(\"Generated Text:\", generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
